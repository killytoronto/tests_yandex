1. **Описание проблемы**

Меня вдохновила задача, которую каждый день решают тысячи русскоязычных пользователей: работа с текстом. Сегодня на российском рынке почти нет полноценных инструментов для глубокого анализа и улучшения текстов на русском языке. То, что представляют собой сервисы вроде TextRu, Advego или eTXT — это в лучшем случае базовая орфография и проверка уникальности. Они не работают со стилем, не "чувствуют" контекста, не помогают сделать текст убедительнее, структурированнее или адаптированным под аудиторию.

При этом на английском рынке уже несколько лет активно развиваются решения вроде Grammarly, QuillBot, Jasper AI — и это становится стандартом. Русскоязычные копирайтеры, журналисты, студенты, исследователи и просто авторы вынуждены использовать устаревшие, малоинтеллектуальные решения, зачастую ещё и платные. Именно это ощущение технологического отставания и стало для меня мотивацией — сделать такой инструмент, который закрывает реальные потребности, а не имитирует полезность.

2. **Решение и реализация**

Я разработал ТекстАИ 2.0 — веб-приложение, которое объединяет сильные стороны нескольких современных языковых моделей и интуитивно понятный интерфейс, полностью заточенный под работу с русским языком.

В основе продукта — HTML/CSS/JavaScript без фреймворков. На фронте реализована поддержка тёмной/светлой темы, адаптивный дизайн, история анализов (через localStorage), быстрые клавиши (Ctrl+Enter), обработка ошибок, и визуальные индикаторы процесса анализа. Запросы отправляются напрямую к API llm7.io, предоставляющему доступ к множеству моделей: GPT-4o, Gemini 1.5 Pro, Mistral Large, Phi-4 и др. Пользователь сам выбирает модель под задачу — от более дешёвых до самых мощных.

Приложение предлагает четыре ключевых режима:

* коррекция текста с улучшением стиля;
* суммаризация длинных текстов;
* извлечение ключевых слов;
* анализ тональности с обоснованием.

Дополнительно реализована функция AI Double Check™ — это вторая нейросеть, которая сравнивает оригинальный и обработанный текст и проверяет, не был ли искажен смысл. Если всё корректно — выводится метка “✓”, если есть проблема — пользователь получает предупреждение и комментарий. Это простой, но надёжный способ контролировать качество генерации.

3. **Прототип и отличие от аналогов**

На данный момент ТекстАИ 2.0 — не идея и не мокап, а работающий продукт, которым можно пользоваться прямо сейчас. Он поддерживает тексты до 5000 символов (лимит обусловлен ограничениями LLM), показывает текущую модель, сохраняет историю, позволяет скопировать результат в один клик, и всё это работает без регистрации и платной подписки.

В отличие от существующих решений:

* TextRu и Advego не работают со стилем, не используют LLM и не адаптируются под пользователя;
* LanguageTool поддерживает русский, но поверхностно и без возможности генерации;
* Яндекс.Спеллер — это орфография и API, пока не продукт.

ТекстАИ 2.0 решает те задачи, которые до сих пор на русском языке были недоступны. При этом делает это с помощью нескольких моделей, с возможностью выбора тона (формальный, креативный, нейтральный, дружелюбный), с анализом на уровне смыслов, а не отдельных слов. И, главное, — бесплатно.

4. **Возможности и ограничения технологий**

Главная сила современных LLM — это их способность понимать контекст и структуру текста, а не просто проверять орфографию. В моём решении используется несколько моделей (GPT-4o, Gemini, Mistral и другие), и каждая из них умеет переформулировать текст, учитывать тональность, делать обобщения и выявлять смысловые блоки. Внутри системы всё построено на диалоге system + user, где первый промпт заранее подготавливает нужный режим (коррекция, суммаризация и т.д.) и стиль речи. Я явно задаю, что модель должна работать на русском языке, с учётом выбранного тона (формальный, креативный и т.п.). Всё это задаётся в функции getSystemPrompt() и инжектится в API-запрос.

Но у этих моделей есть чёткие ограничения, и каждое из них я обработал в коде — не просто "в голове", а в конкретной логике:

Во-первых, лимит по символам. Это напрямую связано с токенизацией. GPT-4o поддерживает большой контекст, но на практике безопасный лимит — 4k–6k токенов на вход и столько же на выход. Я выставил ограничение на 5000 символов. Это значение видно сразу в интерфейсе, а за его соблюдением следит updateCharCount() — если лимит превышен, кнопка анализа блокируется, счётчик становится красным, и система просто не даст отправить текст.

Во-вторых, галлюцинации. LLM могут ошибаться, особенно если промпт неконкретен или вход расплывчат. Это частично решается структурой промпта (я задаю жёсткий системный контекст с инструкциями), но всё равно риски остаются. Именно для этого реализован AI Double Check™: после генерации результат отправляется в ещё один запрос, но уже с задачей сравнить оригинал и результат и выдать вердикт — "ОК" или указание на смысловые искажения. Код этого блока в performAIQualityCheck(), и он активируется только в режиме коррекции, чтобы не перегружать остальные задачи. Вывод — значок “✓” или “⚠️”, а при проблемах — отдельный блок с объяснением, что именно искажено.

Далее, стоимость API. Она напрямую влияет на реализацию. GPT-4o, например, стоит около \$0.01 за 1000 токенов, Gemini — дешевле, Mistral — ещё доступнее. Я намеренно сделал выбор модели открытым: пользователь сам решает, какую модель использовать, и может оптимизировать расходы, если, скажем, работает с большим объёмом. Это реализовано через select с id="modelSelect" и передаётся в теле запроса. Пока проект бесплатный, но в дальнейшем возможна модель монетизации — например, платный доступ к GPT-4o, но бесплатный к Mistral. Все благодаря классному API.

Также есть технические ограничения со стороны браузеров. Например, navigator.clipboard.writeText() может не сработать в браузерах с нестандартными настройками безопасности или в WebView. Поэтому функция copyOutput() сначала проверяет, что есть что копировать, а затем показывает уведомление через showToast(). При ошибке — всплывает сообщение “Ошибка копирования”, и пользователь знает, что можно попробовать в другом браузере.

Скорость отклика тоже зависит от модели. Mistral — самая быстрая, GPT-4o может тормозить при перегрузке. Поэтому в интерфейсе реализован loader, визуально блокирующий область и показывающий процесс. Пользователь всегда видит, когда идёт запрос, и не может повторно нажать кнопку, пока предыдущий не завершён. Это логика в displayOutput(isLoading = true) и в analyzeBtn.disabled = true.

Наконец, есть зависимость от доступности API. Если сервер не отвечает, код пытается распарсить ошибку (try { const errorData = await response.json() ... }) и вывести пользователю текст с объяснением, включая response.status, statusText, или просто text() ответа. Это помогает отловить сбои без “просто не работает”.

Во всех этих случаях при разработке я старался не просто спрятать ошибки, а показать их пользователю максимально понятно — с текстом, который объясняет, в чём именно дело и что можно сделать (например, “сократите текст до 5000 символов”).

В качестве буфера безопасности — реализовано кэширование истории. Все успешные запросы сохраняются в localStorage, и если что-то пойдёт не так — пользователь всегда может вернуться к предыдущему результату. Это делает приложение стабильным даже при кратковременных сбоях с API.

Если дочитали, то спасибо за внимание и я рад, что вы тут!
